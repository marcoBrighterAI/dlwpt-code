{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Exercise\n",
    "Redefine the model to be w2 * t_u ** 2 + w1 * t_u + b.\n",
    "1. What parts of the training loop, and so on, need to change to accommodate this redefinition?\n",
    "2. What parts are agnostic to swapping out the model?\n",
    "3. Is the resulting loss higher or lower after training?\n",
    "4. Is the actual result better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0,\n",
    "                    8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\n",
    "                    33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w1, w2, b):\n",
    "    return (w2 * t_u ** 2) + (w1 * t_u) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'lr_scheduler']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.6181e+00, -2.3049e+02, -5.9642e-02], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.9720e-01,  9.8260e-01, -4.8176e-04], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_un, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad() # <1>\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 5000 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000, Loss 10.704610\n",
      "Epoch 10000, Loss 8.639528\n",
      "Epoch 15000, Loss 7.169393\n",
      "Epoch 20000, Loss 6.122478\n",
      "Epoch 25000, Loss 5.376626\n",
      "Epoch 30000, Loss 4.844937\n",
      "Epoch 35000, Loss 4.465601\n",
      "Epoch 40000, Loss 4.194641\n",
      "Epoch 45000, Loss 4.000774\n",
      "Epoch 50000, Loss 3.861749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.8879,  0.5570, -0.8753], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate) # <1>\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 50000, \n",
    "    optimizer = optimizer,\n",
    "    params = params, # <1> \n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 7, 10,  5,  8,  1,  9,  3,  2,  4]), tensor([6, 0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices  # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
    "                  train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params) # <1>\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "                             \n",
    "        val_t_p = model(val_t_u, *params) # <1>\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward() # <2>\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch <= 3 or epoch % 5000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                  f\" Validation loss {val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 782.4936, Validation loss 195.6477\n",
      "Epoch 2, Training loss 739.9783, Validation loss 188.4566\n",
      "Epoch 3, Training loss 699.8079, Validation loss 181.5967\n",
      "Epoch 5000, Training loss 9.7319, Validation loss 18.2807\n",
      "Epoch 10000, Training loss 8.2674, Validation loss 14.7246\n",
      "Epoch 15000, Training loss 7.1632, Validation loss 12.0005\n",
      "Epoch 20000, Training loss 6.3302, Validation loss 9.9092\n",
      "Epoch 25000, Training loss 5.7016, Validation loss 8.2998\n",
      "Epoch 30000, Training loss 5.2268, Validation loss 7.0579\n",
      "Epoch 35000, Training loss 4.8678, Validation loss 6.0968\n",
      "Epoch 40000, Training loss 4.5961, Validation loss 5.3507\n",
      "Epoch 45000, Training loss 4.3901, Validation loss 4.7695\n",
      "Epoch 50000, Training loss 4.2336, Validation loss 4.3150\n",
      "Epoch 55000, Training loss 4.1144, Validation loss 3.9583\n",
      "Epoch 60000, Training loss 4.0232, Validation loss 3.6770\n",
      "Epoch 65000, Training loss 3.9531, Validation loss 3.4543\n",
      "Epoch 70000, Training loss 3.8990, Validation loss 3.2772\n",
      "Epoch 75000, Training loss 3.8569, Validation loss 3.1356\n",
      "Epoch 80000, Training loss 3.8238, Validation loss 3.0219\n",
      "Epoch 85000, Training loss 3.7975, Validation loss 2.9301\n",
      "Epoch 90000, Training loss 3.7763, Validation loss 2.8556\n",
      "Epoch 95000, Training loss 3.7590, Validation loss 2.7949\n",
      "Epoch 100000, Training loss 3.7447, Validation loss 2.7452\n",
      "Epoch 105000, Training loss 3.7325, Validation loss 2.7043\n",
      "Epoch 110000, Training loss 3.7220, Validation loss 2.6704\n",
      "Epoch 115000, Training loss 3.7127, Validation loss 2.6422\n",
      "Epoch 120000, Training loss 3.7044, Validation loss 2.6186\n",
      "Epoch 125000, Training loss 3.6968, Validation loss 2.5989\n",
      "Epoch 130000, Training loss 3.6898, Validation loss 2.5824\n",
      "Epoch 135000, Training loss 3.6832, Validation loss 2.5683\n",
      "Epoch 140000, Training loss 3.6768, Validation loss 2.5564\n",
      "Epoch 145000, Training loss 3.6708, Validation loss 2.5463\n",
      "Epoch 150000, Training loss 3.6649, Validation loss 2.5375\n",
      "Epoch 155000, Training loss 3.6592, Validation loss 2.5301\n",
      "Epoch 160000, Training loss 3.6535, Validation loss 2.5237\n",
      "Epoch 165000, Training loss 3.6480, Validation loss 2.5181\n",
      "Epoch 170000, Training loss 3.6426, Validation loss 2.5133\n",
      "Epoch 175000, Training loss 3.6372, Validation loss 2.5091\n",
      "Epoch 180000, Training loss 3.6318, Validation loss 2.5053\n",
      "Epoch 185000, Training loss 3.6264, Validation loss 2.5021\n",
      "Epoch 190000, Training loss 3.6212, Validation loss 2.4995\n",
      "Epoch 195000, Training loss 3.6160, Validation loss 2.4968\n",
      "Epoch 200000, Training loss 3.6107, Validation loss 2.4947\n",
      "Epoch 205000, Training loss 3.6055, Validation loss 2.4929\n",
      "Epoch 210000, Training loss 3.6003, Validation loss 2.4911\n",
      "Epoch 215000, Training loss 3.5951, Validation loss 2.4893\n",
      "Epoch 220000, Training loss 3.5898, Validation loss 2.4875\n",
      "Epoch 225000, Training loss 3.5848, Validation loss 2.4863\n",
      "Epoch 230000, Training loss 3.5797, Validation loss 2.4856\n",
      "Epoch 235000, Training loss 3.5746, Validation loss 2.4848\n",
      "Epoch 240000, Training loss 3.5696, Validation loss 2.4841\n",
      "Epoch 245000, Training loss 3.5645, Validation loss 2.4834\n",
      "Epoch 250000, Training loss 3.5595, Validation loss 2.4824\n",
      "Epoch 255000, Training loss 3.5545, Validation loss 2.4815\n",
      "Epoch 260000, Training loss 3.5495, Validation loss 2.4808\n",
      "Epoch 265000, Training loss 3.5445, Validation loss 2.4801\n",
      "Epoch 270000, Training loss 3.5395, Validation loss 2.4794\n",
      "Epoch 275000, Training loss 3.5345, Validation loss 2.4787\n",
      "Epoch 280000, Training loss 3.5295, Validation loss 2.4781\n",
      "Epoch 285000, Training loss 3.5245, Validation loss 2.4774\n",
      "Epoch 290000, Training loss 3.5195, Validation loss 2.4768\n",
      "Epoch 295000, Training loss 3.5146, Validation loss 2.4761\n",
      "Epoch 300000, Training loss 3.5096, Validation loss 2.4755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.9431,  0.5893, -1.7604], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300000, \n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_un, # <1> \n",
    "    val_t_u = val_t_un, # <1> \n",
    "    train_t_c = train_t_c,\n",
    "    val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
