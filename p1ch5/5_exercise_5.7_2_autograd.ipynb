{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Exercise\n",
    "Redefine the model to be w2 * t_u ** 2 + w1 * t_u + b.\n",
    "1. What parts of the training loop, and so on, need to change to accommodate this redefinition?\n",
    "2. What parts are agnostic to swapping out the model?\n",
    "3. Is the resulting loss higher or lower after training?\n",
    "4. Is the actual result better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0,\n",
    "                    3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9,\n",
    "                    33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w1, w2, b):\n",
    "    return (w2 * t_u ** 2) + (w1 * t_u) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5173e+03, 2.6670e+05, 8.2600e+01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()\n",
    "\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:  # <1>\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():  # <2>\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 13.269525\n",
      "Epoch 1000, Loss 12.944254\n",
      "Epoch 1500, Loss 12.629865\n",
      "Epoch 2000, Loss 12.325988\n",
      "Epoch 2500, Loss 12.032271\n",
      "Epoch 3000, Loss 11.748379\n",
      "Epoch 3500, Loss 11.473977\n",
      "Epoch 4000, Loss 11.208752\n",
      "Epoch 4500, Loss 10.952396\n",
      "Epoch 5000, Loss 10.704610\n",
      "Epoch 5500, Loss 10.465109\n",
      "Epoch 6000, Loss 10.233616\n",
      "Epoch 6500, Loss 10.009856\n",
      "Epoch 7000, Loss 9.793580\n",
      "Epoch 7500, Loss 9.584533\n",
      "Epoch 8000, Loss 9.382469\n",
      "Epoch 8500, Loss 9.187162\n",
      "Epoch 9000, Loss 8.998377\n",
      "Epoch 9500, Loss 8.815906\n",
      "Epoch 10000, Loss 8.639528\n",
      "Epoch 10500, Loss 8.469045\n",
      "Epoch 11000, Loss 8.304257\n",
      "Epoch 11500, Loss 8.144971\n",
      "Epoch 12000, Loss 7.991011\n",
      "Epoch 12500, Loss 7.842189\n",
      "Epoch 13000, Loss 7.698338\n",
      "Epoch 13500, Loss 7.559292\n",
      "Epoch 14000, Loss 7.424887\n",
      "Epoch 14500, Loss 7.294972\n",
      "Epoch 15000, Loss 7.169393\n",
      "Epoch 15500, Loss 7.048006\n",
      "Epoch 16000, Loss 6.930671\n",
      "Epoch 16500, Loss 6.817251\n",
      "Epoch 17000, Loss 6.707614\n",
      "Epoch 17500, Loss 6.601639\n",
      "Epoch 18000, Loss 6.499197\n",
      "Epoch 18500, Loss 6.400173\n",
      "Epoch 19000, Loss 6.304451\n",
      "Epoch 19500, Loss 6.211922\n",
      "Epoch 20000, Loss 6.122478\n",
      "Epoch 20500, Loss 6.036016\n",
      "Epoch 21000, Loss 5.952438\n",
      "Epoch 21500, Loss 5.871641\n",
      "Epoch 22000, Loss 5.793540\n",
      "Epoch 22500, Loss 5.718040\n",
      "Epoch 23000, Loss 5.645053\n",
      "Epoch 23500, Loss 5.574501\n",
      "Epoch 24000, Loss 5.506297\n",
      "Epoch 24500, Loss 5.440364\n",
      "Epoch 25000, Loss 5.376625\n",
      "Epoch 25500, Loss 5.315007\n",
      "Epoch 26000, Loss 5.255440\n",
      "Epoch 26500, Loss 5.197854\n",
      "Epoch 27000, Loss 5.142185\n",
      "Epoch 27500, Loss 5.088365\n",
      "Epoch 28000, Loss 5.036334\n",
      "Epoch 28500, Loss 4.986034\n",
      "Epoch 29000, Loss 4.937402\n",
      "Epoch 29500, Loss 4.890388\n",
      "Epoch 30000, Loss 4.844937\n",
      "Epoch 30500, Loss 4.800992\n",
      "Epoch 31000, Loss 4.758508\n",
      "Epoch 31500, Loss 4.717430\n",
      "Epoch 32000, Loss 4.677719\n",
      "Epoch 32500, Loss 4.639323\n",
      "Epoch 33000, Loss 4.602198\n",
      "Epoch 33500, Loss 4.566305\n",
      "Epoch 34000, Loss 4.531602\n",
      "Epoch 34500, Loss 4.498047\n",
      "Epoch 35000, Loss 4.465601\n",
      "Epoch 35500, Loss 4.434231\n",
      "Epoch 36000, Loss 4.403896\n",
      "Epoch 36500, Loss 4.374566\n",
      "Epoch 37000, Loss 4.346202\n",
      "Epoch 37500, Loss 4.318778\n",
      "Epoch 38000, Loss 4.292257\n",
      "Epoch 38500, Loss 4.266613\n",
      "Epoch 39000, Loss 4.241813\n",
      "Epoch 39500, Loss 4.217831\n",
      "Epoch 40000, Loss 4.194641\n",
      "Epoch 40500, Loss 4.172213\n",
      "Epoch 41000, Loss 4.150523\n",
      "Epoch 41500, Loss 4.129546\n",
      "Epoch 42000, Loss 4.109257\n",
      "Epoch 42500, Loss 4.089636\n",
      "Epoch 43000, Loss 4.070658\n",
      "Epoch 43500, Loss 4.052305\n",
      "Epoch 44000, Loss 4.034553\n",
      "Epoch 44500, Loss 4.017383\n",
      "Epoch 45000, Loss 4.000774\n",
      "Epoch 45500, Loss 3.984706\n",
      "Epoch 46000, Loss 3.969166\n",
      "Epoch 46500, Loss 3.954131\n",
      "Epoch 47000, Loss 3.939590\n",
      "Epoch 47500, Loss 3.925519\n",
      "Epoch 48000, Loss 3.911910\n",
      "Epoch 48500, Loss 3.898741\n",
      "Epoch 49000, Loss 3.886002\n",
      "Epoch 49500, Loss 3.873676\n",
      "Epoch 50000, Loss 3.861749\n",
      "Epoch 50500, Loss 3.850209\n",
      "Epoch 51000, Loss 3.839043\n",
      "Epoch 51500, Loss 3.828236\n",
      "Epoch 52000, Loss 3.817780\n",
      "Epoch 52500, Loss 3.807664\n",
      "Epoch 53000, Loss 3.797868\n",
      "Epoch 53500, Loss 3.788390\n",
      "Epoch 54000, Loss 3.779218\n",
      "Epoch 54500, Loss 3.770339\n",
      "Epoch 55000, Loss 3.761743\n",
      "Epoch 55500, Loss 3.753423\n",
      "Epoch 56000, Loss 3.745370\n",
      "Epoch 56500, Loss 3.737571\n",
      "Epoch 57000, Loss 3.730024\n",
      "Epoch 57500, Loss 3.722714\n",
      "Epoch 58000, Loss 3.715637\n",
      "Epoch 58500, Loss 3.708783\n",
      "Epoch 59000, Loss 3.702148\n",
      "Epoch 59500, Loss 3.695721\n",
      "Epoch 60000, Loss 3.689497\n",
      "Epoch 60500, Loss 3.683468\n",
      "Epoch 61000, Loss 3.677629\n",
      "Epoch 61500, Loss 3.671973\n",
      "Epoch 62000, Loss 3.666489\n",
      "Epoch 62500, Loss 3.661180\n",
      "Epoch 63000, Loss 3.656036\n",
      "Epoch 63500, Loss 3.651051\n",
      "Epoch 64000, Loss 3.646214\n",
      "Epoch 64500, Loss 3.641536\n",
      "Epoch 65000, Loss 3.637001\n",
      "Epoch 65500, Loss 3.632601\n",
      "Epoch 66000, Loss 3.628337\n",
      "Epoch 66500, Loss 3.624200\n",
      "Epoch 67000, Loss 3.620189\n",
      "Epoch 67500, Loss 3.616303\n",
      "Epoch 68000, Loss 3.612532\n",
      "Epoch 68500, Loss 3.608877\n",
      "Epoch 69000, Loss 3.605329\n",
      "Epoch 69500, Loss 3.601884\n",
      "Epoch 70000, Loss 3.598553\n",
      "Epoch 70500, Loss 3.595307\n",
      "Epoch 71000, Loss 3.592167\n",
      "Epoch 71500, Loss 3.589117\n",
      "Epoch 72000, Loss 3.586149\n",
      "Epoch 72500, Loss 3.583278\n",
      "Epoch 73000, Loss 3.580482\n",
      "Epoch 73500, Loss 3.577772\n",
      "Epoch 74000, Loss 3.575146\n",
      "Epoch 74500, Loss 3.572583\n",
      "Epoch 75000, Loss 3.570103\n",
      "Epoch 75500, Loss 3.567691\n",
      "Epoch 76000, Loss 3.565336\n",
      "Epoch 76500, Loss 3.563059\n",
      "Epoch 77000, Loss 3.560851\n",
      "Epoch 77500, Loss 3.558690\n",
      "Epoch 78000, Loss 3.556593\n",
      "Epoch 78500, Loss 3.554559\n",
      "Epoch 79000, Loss 3.552577\n",
      "Epoch 79500, Loss 3.550640\n",
      "Epoch 80000, Loss 3.548771\n",
      "Epoch 80500, Loss 3.546946\n",
      "Epoch 81000, Loss 3.545163\n",
      "Epoch 81500, Loss 3.543432\n",
      "Epoch 82000, Loss 3.541750\n",
      "Epoch 82500, Loss 3.540110\n",
      "Epoch 83000, Loss 3.538500\n",
      "Epoch 83500, Loss 3.536946\n",
      "Epoch 84000, Loss 3.535432\n",
      "Epoch 84500, Loss 3.533947\n",
      "Epoch 85000, Loss 3.532497\n",
      "Epoch 85500, Loss 3.531096\n",
      "Epoch 86000, Loss 3.529726\n",
      "Epoch 86500, Loss 3.528385\n",
      "Epoch 87000, Loss 3.527067\n",
      "Epoch 87500, Loss 3.525792\n",
      "Epoch 88000, Loss 3.524552\n",
      "Epoch 88500, Loss 3.523338\n",
      "Epoch 89000, Loss 3.522143\n",
      "Epoch 89500, Loss 3.520971\n",
      "Epoch 90000, Loss 3.519843\n",
      "Epoch 90500, Loss 3.518736\n",
      "Epoch 91000, Loss 3.517653\n",
      "Epoch 91500, Loss 3.516586\n",
      "Epoch 92000, Loss 3.515536\n",
      "Epoch 92500, Loss 3.514527\n",
      "Epoch 93000, Loss 3.513535\n",
      "Epoch 93500, Loss 3.512565\n",
      "Epoch 94000, Loss 3.511603\n",
      "Epoch 94500, Loss 3.510665\n",
      "Epoch 95000, Loss 3.509742\n",
      "Epoch 95500, Loss 3.508854\n",
      "Epoch 96000, Loss 3.507977\n",
      "Epoch 96500, Loss 3.507112\n",
      "Epoch 97000, Loss 3.506263\n",
      "Epoch 97500, Loss 3.505424\n",
      "Epoch 98000, Loss 3.504605\n",
      "Epoch 98500, Loss 3.503812\n",
      "Epoch 99000, Loss 3.503026\n",
      "Epoch 99500, Loss 3.502257\n",
      "Epoch 100000, Loss 3.501494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.1695,  0.6077, -1.1361], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100000, \n",
    "    learning_rate = 1e-5, \n",
    "    params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True), # <1> \n",
    "    t_u = t_un, # <2> \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
